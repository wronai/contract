# Uses LOCAL Ollama instance (running on the host)
# Make sure Ollama is running locally: ollama serve
# Pull required models:
#   ollama pull mistral:7b-instruct
#   ollama pull codellama:7b-instruct

services:
  # Reclapp Studio (Web UI)
  studio:
    build:
      context: .
      dockerfile: Dockerfile
    network_mode: host
    env_file:
      - .env
    volumes:
      - ./projects:/app/projects
      - ../apps:/app/apps:ro
      - ../examples:/app/examples:ro
      - ../dsl:/app/dsl:ro
    environment:
      # With host networking, container can reach host Ollama via localhost.
      STUDIO_HOST: ${STUDIO_HOST:-0.0.0.0}
      STUDIO_PORT: ${STUDIO_PORT:-7860}
      OLLAMA_HOST: ${OLLAMA_HOST:-http://localhost:11434}
      OLLAMA_MODEL: ${OLLAMA_MODEL:-mistral:7b-instruct}
      CODE_MODEL: ${CODE_MODEL:-codellama:7b-instruct}

  # Preview Server (for generated apps)
  preview:
    image: node:20-alpine
    working_dir: /app
    volumes:
      - ./projects:/app/projects
    ports:
      - "3000:3000"
      - "8081:8081"
    command: >
      sh -c "
        if [ -d /app/projects/current/target/api ]; then
          cd /app/projects/current/target/api && npm install && npm run dev &
        fi
        if [ -d /app/projects/current/target/frontend ]; then
          cd /app/projects/current/target/frontend && npm install && npm run dev
        fi
        tail -f /dev/null
      "
    profiles:
      - preview
